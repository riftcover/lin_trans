import json
from pathlib import Path
from typing import Dict, List, Optional, Any

from services.llm_client import ask_gpt
from utils import logger
from utils.agent_dict import agent_msg, AgentConfig


class TerminologyManager:
    """æœ¯è¯­ç®¡ç†å™¨"""

    def __init__(self, custom_terms_path: str = "custom_terms.xlsx"):
        # å½“å‰æœªä½¿ç”¨è‡ªå®šä¹‰æœ¯è¯­åŠŸèƒ½ï¼ŒåæœŸæ·»åŠ åæ‰“å¼€
        # self.custom_terms_path = custom_terms_path
        self.terminology_data = {
            "theme": "",
            "terms": []
        }

    def load_custom_terms(self) -> Dict[str, Any]:
        """åŠ è½½è‡ªå®šä¹‰æœ¯è¯­æ–‡ä»¶"""
        # if not os.path.exists(self.custom_terms_path):
        #     # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
        #     self._create_sample_custom_terms()

        # try:
        # custom_terms = pd.read_excel(self.custom_terms_path)
        # custom_terms_json = {
        #     "terms": [
        #         {
        #             "src": str(row.iloc[0]) if pd.notna(row.iloc[0]) else "",
        #             "tgt": str(row.iloc[1]) if pd.notna(row.iloc[1]) else "",
        #             "note": str(row.iloc[2]) if pd.notna(row.iloc[2]) else ""
        #         }
        #         for _, row in custom_terms.iterrows()
        #         if pd.notna(row.iloc[0]) and str(row.iloc[0]).strip()
        #     ]
        # }
        #     custom_terms_json = {
        #         "src Term": "",
        #         "tgt": "",
        #         "Note": ""
        #     }
        #     print(f"ğŸ“– Custom Terms Loaded: {len(custom_terms_json['terms'])} terms")
        #     return custom_terms_json
        # except Exception as e:
        #     print(f"âš ï¸ Failed to load custom terms: {e}")
        return {"terms": []}

    def generate_summary_and_terminology(self, content: str, agent_name: str,
                                         target_language: str = "ä¸­æ–‡") -> Dict[str, Any]:
        """ç”Ÿæˆå†…å®¹æ€»ç»“å’Œæœ¯è¯­æå–"""

        # é™åˆ¶å†…å®¹é•¿åº¦
        content = content[:2000]

        # åŠ è½½è‡ªå®šä¹‰æœ¯è¯­
        custom_terms = self.load_custom_terms()

        # ç”Ÿæˆæ€»ç»“å’Œæœ¯è¯­æå–çš„æç¤º
        summary_prompt = self._get_summary_prompt(content, custom_terms, target_language)

        # è°ƒç”¨AIç”Ÿæˆæ€»ç»“
        agent_config: AgentConfig = agent_msg[agent_name]

        # å®šä¹‰éªŒè¯å‡½æ•°
        def valid_summary(response_data):
            required_keys = {'src', 'tgt', 'note'}
            if 'terms' not in response_data:
                return {"status": "error", "message": "Missing 'terms' key in response"}
            if 'theme' not in response_data:
                return {"status": "error", "message": "Missing 'theme' key in response"}
            for term in response_data['terms']:
                if any(key not in term for key in required_keys):
                    return {"status": "error", "message": f"Missing required keys in term: {required_keys}"}
            return {"status": "success", "message": "Summary completed"}

        try:
            # ask_gptè°ƒç”¨
            summary_data = ask_gpt(
                agent_config,
                summary_prompt,
                resp_type='json',
                valid_def=valid_summary,
                log_title='terminology_summary'
            )

            # éªŒè¯å“åº”æ ¼å¼
            if not self._validate_summary_response(summary_data):
                raise ValueError("Invalid summary response format")

            # åˆå¹¶è‡ªå®šä¹‰æœ¯è¯­
            if custom_terms["terms"]:
                summary_data["terms"].extend(custom_terms["terms"])

            self.terminology_data = summary_data
            logger.info(f"Generated summary with {len(summary_data['terms'])} terms")
            logger.info("===summary_data===")
            logger.info(summary_data)
            return summary_data

        except Exception as e:
            logger.error(f"Failed to generate summary: {e}")
            # è¿”å›é»˜è®¤ç»“æ„
            default_summary = {
                "theme": f"This appears to be {target_language} subtitle content",
                "terms": custom_terms["terms"]
            }
            self.terminology_data = default_summary
            return default_summary

    def _get_summary_prompt(self, content: str, custom_terms: Dict, target_language: str) -> str:
        """ç”Ÿæˆæ€»ç»“æç¤ºè¯"""
        custom_terms_text = ""
        if custom_terms["terms"]:
            custom_terms_text = "\n\nå·²çŸ¥æœ¯è¯­å‚è€ƒï¼š\n"
            for term in custom_terms["terms"]:
                custom_terms_text += f"- {term['src']} â†’ {term['tgt']} ({term['note']})\n"

        prompt = f"""è¯·åˆ†æä»¥ä¸‹å­—å¹•å†…å®¹ï¼Œæå–å…³é”®æœ¯è¯­å¹¶ç”Ÿæˆä¸»é¢˜æè¿°ã€‚

å†…å®¹ï¼š
{content}

{custom_terms_text}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
{{
    "theme": "å†…å®¹ä¸»é¢˜çš„ç®€çŸ­æè¿°ï¼ˆç”¨{target_language}ï¼‰",
    "terms": [
        {{
            "src": "æºè¯­è¨€æœ¯è¯­",
            "tgt": "{target_language}ç¿»è¯‘",
            "note": "æœ¯è¯­è¯´æ˜æˆ–ä¸Šä¸‹æ–‡"
        }}
    ]
}}

è¦æ±‚ï¼š
1. themeåº”è¯¥æ˜¯å¯¹å†…å®¹ä¸»é¢˜çš„ç®€æ´æè¿°
2. æå–5-15ä¸ªé‡è¦æœ¯è¯­
3. æœ¯è¯­åº”åŒ…æ‹¬ä¸“ä¸šè¯æ±‡ã€äººåã€åœ°åã€å“ç‰Œåç­‰
4. ç¿»è¯‘è¦å‡†ç¡®ä¸”ç¬¦åˆ{target_language}ä¹ æƒ¯
5. noteæä¾›å¿…è¦çš„ä¸Šä¸‹æ–‡è¯´æ˜

è¯·ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"""

        return prompt

    def _validate_summary_response(self, response_data: Any) -> bool:
        """éªŒè¯æ€»ç»“å“åº”æ ¼å¼"""
        if not isinstance(response_data, dict):
            return False

        if "theme" not in response_data or "terms" not in response_data:
            return False

        if not isinstance(response_data["terms"], list):
            return False

        required_keys = {"src", "tgt", "note"}
        for term in response_data["terms"]:
            if not isinstance(term, dict):
                return False
            if not all(key in term for key in required_keys):
                return False

        return True

    def search_terms_in_sentence(self, sentence: str) -> Optional[str]:
        """åœ¨å¥å­ä¸­æœç´¢ç›¸å…³æœ¯è¯­ï¼ˆæ¨¡æ‹Ÿsearch_things_to_note_in_promptï¼‰"""
        if not self.terminology_data["terms"]:
            return None

        found_terms = []
        sentence_lower = sentence.lower()

        for term in self.terminology_data["terms"]:
            src_term = term["src"].lower()
            if src_term in sentence_lower:
                found_terms.append(term)

        if found_terms:
            prompt_lines = []
            for i, term in enumerate(found_terms):
                prompt_lines.append(
                    f'{i + 1}. "{term["src"]}": "{term["tgt"]}", meaning: {term["note"]}'
                )
            return '\n'.join(prompt_lines)

        return None

    def save_terminology(self, filepath: str):
        """ä¿å­˜æœ¯è¯­æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            filepath_obj = Path(filepath)
            # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
            filepath_obj.parent.mkdir(parents=True, exist_ok=True)
            
            with open(filepath_obj, 'w', encoding='utf-8') as f:
                json.dump(self.terminology_data, f, ensure_ascii=False, indent=4)
            logger.info(f"æœ¯è¯­æ•°æ®å·²ä¿å­˜åˆ°: {filepath_obj}")
        except Exception as e:
            logger.error(f"ä¿å­˜æœ¯è¯­æ•°æ®å¤±è´¥: {e}")

    def load_terminology(self, filepath: str) -> bool:
        """ä»æ–‡ä»¶åŠ è½½æœ¯è¯­æ•°æ®"""
        try:
            filepath_obj = Path(filepath)
            if filepath_obj.exists():
                with open(filepath_obj, 'r', encoding='utf-8') as f:
                    self.terminology_data = json.load(f)
                logger.info(f"ğŸ“– æœ¯è¯­æ•°æ®å·²ä»æ–‡ä»¶åŠ è½½: {filepath_obj}")
                return True
            return False
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æœ¯è¯­æ•°æ®å¤±è´¥: {e}")
            return False

    def get_theme(self) -> str:
        """è·å–å†…å®¹ä¸»é¢˜"""
        return self.terminology_data.get("theme", "")

    def get_terms(self) -> List[Dict[str, str]]:
        """è·å–æ‰€æœ‰æœ¯è¯­"""
        return self.terminology_data.get("terms", [])

    def add_custom_term(self, src: str, tgt: str, note: str = ""):
        """æ·»åŠ è‡ªå®šä¹‰æœ¯è¯­"""
        new_term = {"src": src, "tgt": tgt, "note": note}
        self.terminology_data["terms"].append(new_term)

    # def update_custom_terms_file(self, terms: List[Dict[str, str]]):
    #     """æ›´æ–°è‡ªå®šä¹‰æœ¯è¯­æ–‡ä»¶"""
    #     try:
    #         df_data = {
    #             "Source Term": [term["src"] for term in terms],
    #             "Target Translation": [term["tgt"] for term in terms],
    #             "Note": [term["note"] for term in terms]
    #         }
    #         df = pd.DataFrame(df_data)
    #         df.to_excel(self.custom_terms_path, index=False)
    #         print(f"ğŸ’¾ Updated custom terms file: {self.custom_terms_path}")
    #     except Exception as e:
    #         print(f"âŒ Failed to update custom terms file: {e}")


def create_terminology_for_content(content: str, agent_name: str,
                                   target_language: str = "ä¸­æ–‡",
                                   custom_terms_path: str = "custom_terms.xlsx") -> TerminologyManager:
    """ä¸ºå†…å®¹åˆ›å»ºæœ¯è¯­ç®¡ç†å™¨çš„ä¾¿æ·å‡½æ•°"""
    manager = TerminologyManager(custom_terms_path)
    manager.generate_summary_and_terminology(content, agent_name, target_language)
    return manager


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    sample_content = """
    Welcome to this machine learning tutorial. Today we'll discuss neural networks,
    artificial intelligence, and deep learning algorithms. We'll cover topics like
    gradient descent, backpropagation, and convolutional neural networks.
    """

    manager = create_terminology_for_content(
        content=sample_content,
        agent_name="qwen_cloud",
        target_language="ä¸­æ–‡"
    )

    # æµ‹è¯•æœ¯è¯­æœç´¢
    # test_sentence = "Neural networks are fundamental to artificial intelligence"
    # terms_prompt = manager.search_terms_in_sentence(test_sentence)
    # if terms_prompt:
    #     print("Found terms:")
    #     print(terms_prompt)
